---
title: "Estimating_valences"
author: "Christian Rohrsen"
date: "5 Juni 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries 

We first upload some libraries and fetch/create some functions

```{r cars}
library(MASS)
library(ISLR)
library(nlme)
library(lme4)
require(gclus)
library(rgl)
source("http://www.phaget4.org/R/myImagePlot.R") 

regplot=function(x,y){
  
  fit=lm(y~x)
  plot(x,y)
  abline(fit,col="red")
}

```

## Entering data by hand

For the fly brain (Drosophila melanogaster) we have the GAL4-UAS system. For that we clone the Gal4 right after a promoter that expresses in a brain region of interest. This will trigger the expression of the Gal4 protein in the desired neurons. In addition, we have another construct cloned into the fly DNA, the UAS (it is a promoter that is controlled by the binding of GAL4) followed by Chrimson (which is a channel that opens with light). Therefore, if we have a fly with TH-G4 (tyrosine hidroxylase promoter, expresses in dopaminergic neurons. G4 protein is then present in the cytosol of dopaminergic neurons). Then this G4 will bind to the UAS region, triggering the expression of Chrimson. Dopaminergic neurons will have light sensitive channels in their membranes, that depolarizes the neurons once we display light on the fly heads.

We have a collection of driver lines (GAL4s) that expresses in different subsets of dopaminergic neurons. Their expression pattern in the brain is described in the following tables
```{r}
## Inputting the expression table for all the G4s by hand. Normalized from 0 to 1.
#data<-c(rep(0.9,8),0.6,0.3,0,0.9,0,0.3,0.9,0.9,0.6,rep(0.9,7),0.3,rep(0,8),0.3,0.3,rep(0,7),0.6,0.9,0.3,0.6,0.9,0.9,0.9,0.6,0.6,0,0.6,rep(0,6),0.6,0.9,0.6,0.9,0.6,0.9,0.6,0.9,0.6,0.9,0.6,0.9,0.9,0.9,0.9,rep(0,13),0.9,0.9,0.6,0.6,rep(0,13),0.6,0.6,0.6,rep(0,14),0.3,0.3,0.3,rep(0,14),0.9,0.9,0.9,rep(0,12),0.9,0.6,rep(0,15),0.3,0.3,rep(0,15),0.9,0.6,rep(0,15),0.9,0.6,rep(0,15),rep(0,14),0.3,0.6,rep(0,10),0.6,0,0,0,0.6,0,0,0,0,0.6,0,0,0,0,0,0,0,0.9,0.9,0.6,rep(0,13),0.9,0.3,rep(0,15),0.9,0.3,rep(0,7))

#expression<- matrix(data,17,18)
#expression<-t(expression)

## Setting all the names of for the G4s and the Mushroom Bodies projecting sites for the table created above
#colnames(expression)<-c("y1(PAM,PPL1)","ped(PAM,PPL1)","a2(PPL1)","a'2(PPL1)","a'1(PPL1)","y2(PPL1,PAM)","a3(PPL1)","a'3(PPL1)","b2(PAM)","b'2(PAM)","b1(PAM)","a1(PAM)","b'1(PAM)","y5(PAM)","y4(PAM)","y3(PAM)","ca(PPL2ab)")
#rownames(expression)<-c("TH-G4","TH-G4+Cha-G80","DDC-G4(HL8)","DDC-G4(HL9)","MB-G80+NP47-G4","5htr1b-G4","5htr1b-G4+Cha-G80","NP7187-G4","MZ840-G4","c061-G4+MB-G80","c061-G4+MB-G80+Cha-G80","c259-G4+MB-G80","NP2758-G4","NP7323-G4","MZ19-G4+Cha-G80","NP6510-G4","NP5272-G4","NP1528-G4")

data<-c(rep(0.9,8),0.6,0.3,0,0.9,0,0.3,0.9,0.9,0.6,rep(0.9,7),0.3,rep(0,8),0.3,0.3,rep(0,7),0.6,0.9,0.3,0.6,0.9,0.9,0.9,0.6,0.6,0,0.6,rep(0,6),0.6,0.9,0.6,0.9,0.6,0.9,0.6,0.9,0.6,0.9,0.6,0.9,0.9,0.9,0.9,rep(0,13),0.9,0.9,0.6,0.6,rep(0,13),0.6,0.6,0.6,rep(0,14),0.3,0.3,0.3,rep(0,14),0.9,0.9,0.9,rep(0,12),0.9,0.6,rep(0,15),0.3,0.3,rep(0,15),0.9,0.6,rep(0,15),0.9,0.6,rep(0,15),rep(0,14),0.3,0.6,rep(0,10),0.6,0,0,0,0.6,0,0,0,0,0.6,0,0,0,0,0,0,0,0.9,0.9,0.6,rep(0,13),0.9,0.3,rep(0,15),0.9,0.3,rep(0,7),rep(0,12),0.5,rep(0,13),0.5,rep(0,16),0.7,rep(0,9),0.1,0.1,rep(0,19),0.1,rep(0,16),0.1,rep(0,19),0.5,rep(0,18),0.3,rep(0,13),0.3,0.3,rep(0,14),0.1,rep(0,22),0.9,rep(0,3),0.1,rep(0,20),0.1,0.1,rep(0,11),0.9,0.9,0,0,0,0.9,0,0,0.9,0.9,0.9,0.9,0.3,0.9,0.9,0.3,rep(0,9),0.3,0.6,0.9,0,0.3,0.6,0.9,0.3,0,0,0,0.9,0.9,0.9,0,0,0,0.9,0.3,rep(0,8),0.6,rep(0,6),0.9,0.9,0.9,0.6,rep(0,6),0.6,0.9,0.9,0.9,0,0,0,0,0.9,0.9,0.6,0,0,0,0,0)
expression<- matrix(data,17,36)
expression<-t(expression)

#expression(paste("Sampled values, ", mu, "=5, ", sigma,"=1"))

colnames(expression)<-c("y1(PAM,PPL1)","ped(PAM,PPL1)","a2(PPL1)","a'2(PPL1)","a'1(PPL1)","y2(PPL1,PAM)","a3(PPL1)","a'3(PPL1)","b2(PAM)","b'2(PAM)","b1(PAM)","a1(PAM)","b'1(PAM)","y5(PAM)","y4(PAM)","y3(PAM)","ca(PPL2ab)")
rownames(expression)<-c("TH-G4","TH-G4+Cha-G80","DDC-G4(HL8)","DDC-G4(HL9)","MB-G80+NP47-G4","5htr1b-G4","5htr1b-G4+Cha-G80","NP7187-G4","MZ840-G4","c061-G4+MB-G80","c061-G4+MB-G80+Cha-G80","c259-G4+MB-G80","NP2758-G4","NP7323-G4","MZ19-G4+Cha-G80","NP6510-G4","NP5272-G4","NP1528-G4","mb025b-G4","mb032b-G4","mb056b-G4","mb058b-G4","mb060b-G4","mb065b-G4","mb109b-G4","mb299b-G4","mb301b-G4","mb304b-G4","mb315c-G4","mb438b-G4","mb439b-G4","58E02-G4","58E02-G4+th-G80","np5272-G4+mz840-G4","np6510-G4+np5272-G4","np6510-G4+mz840-G4")


#testedflies<-expression[c(1,3,4,5,6,9,12,15,16,17,18),]
testedflies<-expression

library(raster)
r <- raster(testedflies)
plot(r, col = gray.colors(10, start = 1, end = 0, gamma = 2.2, alpha = NULL))

myImagePlot(testedflies)

```

## Driver lines expression

This is just to see how the driver lines expression correlate. This is important because if there are regions that only express with other regions, it is difficult to find out their effects alone (without the correlated region). 
```{r}

## Table correlating expression of brain regions in the available G4s

my.abs     <- abs(cor(testedflies))
#my.colors  <- dmat.color(my.abs)
my.colors2  <- dmat.color(my.abs, colors = c(rgb(0,0.9,0),rgb(0,0.6,0),rgb(0,0.4,0)))
my.ordered <- order.single(cor(testedflies))
cpairs(testedflies, my.ordered, panel.colors=my.colors2, gap=0.5)

## Setting a threshold to see which regions are highly correlated: that means expressed in the same G4s
#bestcorr<-unique(my.abs[my.abs>0.8])
#bestcorr

```


A Principal Component analysis uses the linear correlations in expression to make new more compact components. We can see which regions are more or less correlated in their expression among the different G4s. We can also see how the different driver lines map in the three first PCs

```{r}
# Do the PCA 

my.prc <- prcomp(testedflies, center=TRUE, scale=FALSE)
screeplot(my.prc, main="Scree Plot", xlab="Components")
screeplot(my.prc, main="Scree Plot", type="line" )

# DotPlot PC1

load    <- my.prc$rotation
sorted.loadings <- load[order(load[, 1]), 1]
myTitle <- "Loadings Plot for PC1" 
myXlab  <- "Variable Loadings"
dotchart(sorted.loadings, main=myTitle, xlab=myXlab, cex=1, col="red")

# DotPlot PC2

sorted.loadings <- load[order(load[, 2]), 2]
myTitle <- "Loadings Plot for PC2"
myXlab  <- "Variable Loadings"
dotchart(sorted.loadings, main=myTitle, xlab=myXlab, cex=1, col="red")

# Now draw the BiPlot
biplot(my.prc, cex=c(0.5, 0.7))



pca.scores<-NULL
scores<-NULL
for (i in 1:ncol(my.prc$rotation)){
scores<- apply(testedflies,1,function(x)sum(x*my.prc$rotation[,i]))
pca.scores<-cbind(pca.scores,scores)
}

pca.scores<-as.data.frame(pca.scores)
colnames(pca.scores)<-c("PC1","PC2","PC3","PC4","PC5","PC6","PC7","PC8","PC9","PC10","PC11")

# Plot the driver lines in the three first PCs
plot3d(pca.scores[,1:3], col=length(rownames(testedflies)),size=10)

text3d(pca.scores[,1:3],texts=rownames(testedflies),cex=0.7,font=2)
text3d(my.prc$rotation[,1:3], texts=attributes(my.prc$rotation)$dimnames[[1]], col="red",cex=0.7,font=2)

coords <- NULL
for (i in 1:nrow(my.prc$rotation)) {
  coords <- rbind(coords, rbind(c(0,0,0),my.prc$rotation[i,1:3]))
}
lines3d(coords, col="red", lwd=1)

```

This is just for playing around. I have made a K-means to cluster the driver lines that have more similar expression.

```{r}
# Apply the Varimax Rotation
my.var <- varimax(my.prc$rotation)

# Do a K means just to see how similar the expression of different driver lines are
set.seed(42)
cl <- kmeans(testedflies,10)
cluster <- as.factor(cl$cluster)

# Plot k-means in 3D
plot3d(pca.scores[,1:3], col=cluster, main="k-means clusters",size=10)
text3d(pca.scores[,1:3],texts=rownames(testedflies),cex=0.7,font=2)

```


This is just for playing around. This is the same as above but clustering is done in a hierarchical dendrogram.

```{r}
## This is a Tree clustering to see how different each driver line expression pattern is
di <- dist(testedflies, method="euclidean")
tree <- hclust(di, method="ward.D")
hierarchical.cluster <- as.factor((cutree(tree, k=10)-2) %% 3 +1)
# that modulo business just makes the coming table look nicer
plot(tree, xlab="")
rect.hclust(tree, k=10, border="red")

```

## Importing data with behavioral scores

We can import the results from the behavioral experiments of these flies. For each driver line available we expressed the light sensitive channel Chrimson. Therefore, when the flies were in the light, the given subset of dopaminergic neurons turned on. If the flies "liked/enjoyed" the activation of these neurons, they will stay in the light. If they do not "like/enjoy" the activation of these neurons they will go to the dark. In this way we can estimate in neurons are positively reinforcing or not. So we get scores were positive numbers means they approach the light, whereas negative numbers indicate they avoid the light, they do not like the "feeling" they get when they activate these neurons (the ones activated by the corresponding driver line, GAL4)

We have different experiments.

Here the T-maze
```{r}

## Importing the data measured from the flies targeting the different driver lines

setwd("C:/Users/LocalAdmin/Desktop")

################## Tmaze data import  ###############################

Tmaze <- read.csv(file.choose(), header = TRUE, sep = ";", quote = "\"",dec = "," )
colnames(Tmaze)[1] <- "Fly.line"
nExp <- length (Tmaze[[1]])

###### A less efficient way of calculating PIs

Tmaze$PI <- vector("numeric", length = nExp)
for(i in 1:nExp){
  Tmaze$PI[i] <- (Tmaze[[i,2]]-Tmaze[[i,4]])/(Tmaze[[i,2]]+Tmaze[[i,4]])                 
}

###### This is in order to make groups according to their names 

idGroup <- data.frame("Group"=levels(Tmaze[[1]]))


### makemeans for the groups in the idGroup table.

idGroup$mean <- NULL
mean <- NULL

idGroup$mean <- sapply(seq_len(nrow(idGroup)), function(i) { 
  mean(Tmaze$PI[idGroup$Group[i]==Tmaze$Fly.line])
})
```

Here the Y-mazes
```{r}
############### Y-mazes import #################################

## Import the scores with rows (fly), columns (driver line)
scores <- read.delim("C:/Users/LocalAdmin/Desktop/data/Harvard data/screen/effect_occupancy_compact.txt", header=FALSE)

line_names <- read.table("C:/Users/LocalAdmin/Desktop/data/Harvard data/screen/line_names.txt", quote="\"", comment.char="")

colnames(scores)<- line_names$V1

for (i in 1:48){
  line_names$scores[i] <- mean(scores[,i],na.rm = TRUE)
  line_names$sd[i] <- sd(scores[,i],na.rm = TRUE)
  line_names$n[i] <- length(scores[!is.na(scores[,i]),i])
}

line_names$se <- line_names$sd / sqrt(line_names$n)



```

## Modify the the matrix in underrank cases

This is just when the number of lines tested (N) are less than the number of brain regions for which we want to know their valence/reinforcement (M)
```{r}
# This is to create testedflies-shrink which takes the driver lines that are correlated, to reduce unknown variables in under-ranked conditions
testedflies.shrink<- testedflies

shrink1<-c(testedflies.shrink[,3],testedflies.shrink[,4],testedflies.shrink[,5],testedflies.shrink[,6])
shrink2<-c(testedflies.shrink[,12],testedflies.shrink[,15],testedflies.shrink[,16],testedflies.shrink[,17])

testedflies.shrink[,3]<-apply(testedflies.shrink[,3:6],1,mean)
testedflies.shrink[,12]<-apply(testedflies.shrink[,c(12,15,16,17)],1,mean)

testedflies.shrink<-subset(testedflies.shrink,select=c(1:3,7:14))
```

## Order the scores and expression so that they match

Since I did not test behaviorally all the driver lines for which I have the expression, I have to subset them, and reorder them to match the behavioral scores 
```{r}

expression_ordered <- rbind(expression[6,],expression[7,],expression[32,],expression[33,],expression[3,],expression[19:31,],expression[15,],expression[9,],expression[36,],expression[5,],expression[18,],expression[13,],expression[17,],expression[34,],expression[16,],expression[35,],expression[2,])

colnames(expression_ordered)<-c("y1(PAM,PPL1)","ped(PAM,PPL1)","a2(PPL1)","a'2(PPL1)","a'1(PPL1)","y2(PPL1,PAM)","a3(PPL1)","a'3(PPL1)","b2(PAM)","b'2(PAM)","b1(PAM)","a1(PAM)","b'1(PAM)","y5(PAM)","y4(PAM)","y3(PAM)","ca(PPL2ab)")
rownames(expression_ordered)<-c(rownames(expression)[6],rownames(expression)[7],rownames(expression)[32],rownames(expression)[33],rownames(expression)[3],rownames(expression)[19:31],rownames(expression)[15],rownames(expression)[9],rownames(expression)[36],rownames(expression)[5],rownames(expression)[18],rownames(expression)[13],rownames(expression)[17],rownames(expression)[34],rownames(expression)[16],rownames(expression)[35],rownames(expression)[2])

names_ordered <- line_names$V1[c(-5,-20,-24,-25,-26,-34,-35,-36,-37,-38,-39,-40,-41,-42,-43,-45,-46,-47,-48)]
scores_ordered <- line_names$scores[c(-5,-20,-24,-25,-26,-34,-35,-36,-37,-38,-39,-40,-41,-42,-43,-45,-46,-47,-48)]

scores_ordered_lin<-unlist(scores[,c(-5,-20,-24,-25,-26,-34,-35,-36,-37,-38,-39,-40,-41,-42,-43,-45,-46,-47,-48)])


r <- raster(expression_ordered)
plot(r, col = gray.colors(10, start = 1, end = 0, gamma = 2.2, alpha = NULL))

myImagePlot(expression_ordered)


```

## Resolve the linear differential equations

```{r}
# Solving the linear differential equation with the means
#DANs.valence<-t(testedflies.shrink)%*%PIs
DANs.valence<-t(expression_ordered)%*%scores_ordered
barplot(t(DANs.valence),names.arg=rownames(DANs.valence),las=2,ylim=c(-1,1),mar = c(10, 5, 4, 2))

# Doing a prediction
expected.model<-apply(expression_ordered,1,function(x) sum(x*DANs.valence))

# Plotting the expected (fit) vs the observed PI scores
expected_observed<-rbind(expected.model,scores_ordered)
rownames(expected_observed)<-c("expected","observed")

plot(expected_observed[1,],ylim=c(-1,1),mar = c(10, 5, 4, 2),ylab="PI",xlab="",main="Observed vs Modelled PIs for the dopaminergic subsets")
points(expected_observed[2,],col="red")
segments(x0 = 0, y0 = 0, x1 = 30, y1 = 0, col = "blue", lwd = 1)
legend(x=1,legend=c("Expected","Observed"),col=c("black","red"),text.col = c("black","red"), pch = c(1, 1),
        bg = "gray90")
```
## Resolve the linear differential equations without making means before but with the whole data so that it finds its best fit

```{r}

## Solving with each singular fly value. The least mean squares should be solved separatedly

#This is to get the expression values and the behavioral scores for each single experiment
expression_individual <- matrix(NA,nrow = 120*nrow(expression_ordered),ncol = 17)
for (i in 1:ncol(expression_ordered)){
expression_individual[,i] <- rep(expression_ordered[,i],each = 120,length.out = 120*nrow(expression_ordered), times=1)
}

scores_indiv <- scores_ordered_lin[!is.nan(scores_ordered_lin)]
data_indiv <- expression_individual[!is.nan(scores_ordered_lin),]
#colnames(data_indiv)<-rownames(DANs.valence)
# Making a data frame with scores and expression together
df<-data.frame(cbind(data_indiv,scores_indiv))
#colnames(df)<- c("y1(PAM,PPL1)","ped(PAM,PPL1)","a2(PPL1)","aPRIME2(PPL1)","aPRIME1(PPL1)","y2(PPL1,PAM)","a3(PPL1)","aPRIME3(PPL1)","b2(PAM)","bPRIME2(PAM)","b1(PAM)","a1(PAM)","bPRIME1(PAM)","y5(PAM)","y4(PAM)","y3(PAM)","ca(PPL2ab)")  


# Solve the equations
DANs.valence2<-t(data_indiv)%*%scores_indiv

barplot(t(DANs.valence2),names.arg=rownames(DANs.valence),las=2,mar = c(10, 5, 4, 2))

# Doing a prediction
expected.model2<-apply(expression_ordered,1,function(x) sum(x*DANs.valence2))

# Plotting the expected (fit) vs the observed PI scores
expected_observed2<-rbind(expected.model2,scores_ordered)
rownames(expected_observed2)<-c("expected","observed")

plot(expected_observed2[1,],ylim=c(-1,1),mar = c(10, 5, 4, 2),ylab="PI",xlab="",main="Observed vs Modelled PIs for the dopaminergic subsets")
points(expected_observed2[2,],col="red")
segments(x0 = 0, y0 = 0, x1 = 30, y1 = 0, col = "blue", lwd = 1)
legend(x=1,legend=c("Expected","Observed"),col=c("black","red"),text.col = c("black","red"), pch = c(1, 1),
        bg = "gray90")

```
## Resolve the linear differential equations without making means before but with the whole data so that it finds its best fit. With lm

```{r}

## Solving with each singular fly value. The least mean squares should be solved separatedly

# With interactions it yields better Rsquared/Likelihood/AIC/BIC scores. So we comment the model without interactions
#linear_model3_without<- lm(scores_indiv ~ (data_indiv[,1]+data_indiv[,2]+data_indiv[,3]+data_indiv[,4]+data_indiv[,5]+data_indiv[,6]+data_indiv[,7]+data_indiv[,8]+data_indiv[,9]+data_indiv[,10]+data_indiv[,11]+data_indiv[,12]+data_indiv[,13]+data_indiv[,14]+data_indiv[,15]+data_indiv[,16]+data_indiv[,17]))

linear_model3<- lm(scores_indiv ~ (data_indiv[,1]+data_indiv[,2]+data_indiv[,3]+data_indiv[,4]+data_indiv[,5]+data_indiv[,6]+data_indiv[,7]+data_indiv[,8]+data_indiv[,9]+data_indiv[,10]+data_indiv[,11]+data_indiv[,12]+data_indiv[,13]+data_indiv[,14]+data_indiv[,15]+data_indiv[,16]+data_indiv[,17])^2)


#Null Deviance = 2(LL(Saturated Model) - LL(Null Model)) on df = df_Sat - df_Null
#Residual Deviance = 2(LL(Saturated Model) - LL(Proposed Model)) df = df_Sat - df_Proposed
#What about the Fisher scoring algorithm? Fisher's scoring algorithm is a derivative of Newton's method for solving maximum likelihood problems numerically.
#For model1 we see that Fisher's Scoring Algorithm needed six iterations to perform the fit.
#This doesn't really tell you a lot that you need to know, other than the fact that the model did indeed converge, and had no trouble doing it.
summary(linear_model3)
#summary(linear_model3_without)

barplot(linear_model3$coefficients,names.arg=attr(linear_model3$coefficients,"names"),las=2,mar = c(10, 5, 4, 2))

## Significant are 2,3,4,6,7,9,11,23,35,44,45,124
barplot(c(linear_model3$coefficients[2],linear_model3$coefficients[3],linear_model3$coefficients[4],linear_model3$coefficients[6],linear_model3$coefficients[7],linear_model3$coefficients[9],linear_model3$coefficients[11],linear_model3$coefficients[23],linear_model3$coefficients[35],linear_model3$coefficients[44],linear_model3$coefficients[45],linear_model3$coefficients[124]),names.arg=c(attr(linear_model3$coefficients,"names")[2],attr(linear_model3$coefficients,"names")[3],attr(linear_model3$coefficients,"names")[4],attr(linear_model3$coefficients,"names")[6],attr(linear_model3$coefficients,"names")[7],attr(linear_model3$coefficients,"names")[9],attr(linear_model3$coefficients,"names")[11],attr(linear_model3$coefficients,"names")[23],attr(linear_model3$coefficients,"names")[35],attr(linear_model3$coefficients,"names")[44],attr(linear_model3$coefficients,"names")[45],attr(linear_model3$coefficients,"names")[124]),las=2,mar = c(10, 5, 4, 2))

#Generic function calculating Akaike's 'An Information Criterion' for one or several fitted model objects for which a log-likelihood value can be obtained, according to the formula -2*log-likelihood + k*npar, where npar represents the number of parameters in the fitted model, and k = 2 for the usual AIC, or k = log(n) (n being the number of observations) for the so-called BIC or SBC (Schwarz's Bayesian criterion). 
AIC(linear_model3)  ## -1786.064
stopifnot(all.equal(AIC(linear_model3),
                    AIC(logLik(linear_model3)))) ## logLik = 919.0322 (df =26)
BIC(linear_model3)   ## -1629.223

#AIC(linear_model3_without)   ## -1600.953
#stopifnot(all.equal(AIC(linear_model3_without),
#                    AIC(logLik(linear_model3_without))))  ## logLik = 818.4766 (df=18)
#BIC(linear_model3_without)   ## -1492.371

#general_linear_model <- glm(scores_indiv ~  (V1+V2+V3+V4+V5+V6+V7+V8+V9+V10+V11+V12+V13+V14+V15+V16+V17)^2, data = df,family = gaussian(link = "identity"))
#plot(testedflies.shrink)
#lines(testedflies.shrink, exp(general_linear_model$fitted), col = 2, lwd = 2)
#summary(general_linear_model)# display results
#confint(general_linear_model) # 95% CI for the coefficients
#exp(coef(general_linear_model)) # exponentiated coefficients
#exp(confint(general_linear_model)) # 95% CI for exponentiated coefficients
#predict(general_linear_model, type="response") # predicted values
#residuals(general_linear_model, type="deviance") # residuals
#get some estimation of goodness of fit
#cor(PIs,predict(general_linear_model))

## another multivariate model
#m1 <- glmer(scores_indiv ~  V1+V2+V3+V4+V5,data = df, family="gaussian")
#summary(m1)


```

## Bayesian estimation

```{r}

#*************************************************************Error************************************************************************

#library(MCMCpack)

#bayes_model <- MCMCregress(scores_ordered ~  expression_ordered[,1] + expression_ordered[,2] + expression_ordered[,3] + expression_ordered[,4] + expression_ordered[,5] + expression_ordered[,6] + expression_ordered[,7] + expression_ordered[,8] + expression_ordered[,9] + expression_ordered[,10] + expression_ordered[,11] + expression_ordered[,12] + expression_ordered[,13] + expression_ordered[,14] + expression_ordered[,15] + expression_ordered[,16] + expression_ordered[,17])

#bayes_model <- MCMCregress(scores_indiv ~ data_indiv[,1]+data_indiv[,2]+data_indiv[,3]+data_indiv[,4]+data_indiv[,5]+data_indiv[,6]+data_indiv[,7]+data_indiv[,8]+data_indiv[,9]+data_indiv[,10]+data_indiv[,11]+data_indiv[,12]+data_indiv[,13]+data_indiv[,14]+data_indiv[,15]+data_indiv[,16]+data_indiv[,17])

#summary(bayes_model)
#plot(bayes_model)
#HPDinterval(bayes_model)
#effectiveSize(bayes_model)
#raftery.diag(bayes_model)
#acf(bayes_model)

#*************************************************************Error************************************************************************

library(arm)

#bayes_model2<-bayesglm(scores_ordered ~ expression_ordered[,1] + expression_ordered[,1] + expression_ordered[,2] + expression_ordered[,3] + expression_ordered[,4] + expression_ordered[,5] + expression_ordered[,6] + expression_ordered[,7] + expression_ordered[,8] + expression_ordered[,9] + expression_ordered[,10] + expression_ordered[,11] + expression_ordered[,12] + expression_ordered[,13] + expression_ordered[,14] + expression_ordered[,15] + expression_ordered[,16] + expression_ordered[,17])
#summary(bayes_model2)
#barplot(bayes_model2$coefficients,names.arg=c("Incercept",rownames(DANs.valence)),las=2,ylim=c(-1,1),mar = c(10, 5, 4, 2))

#Bayesian functions for generalized linear modeling with independent normal, t, or Cauchy prior distribution for the coefficients.The program is a simple alteration of glm() that uses an approximate EM algorithm to update the betas at each step using an augmented regression to represent the prior information.
bayes_model3_without<-bayesglm(scores_indiv ~ (data_indiv[,1]+data_indiv[,2]+data_indiv[,3]+data_indiv[,4]+data_indiv[,5]+data_indiv[,6]+data_indiv[,7]+data_indiv[,8]+data_indiv[,9]+data_indiv[,10]+data_indiv[,11]+data_indiv[,12]+data_indiv[,13]+data_indiv[,14]+data_indiv[,15]+data_indiv[,16]+data_indiv[,17]))

bayes_model3<-bayesglm(scores_indiv ~ (data_indiv[,1]+data_indiv[,2]+data_indiv[,3]+data_indiv[,4]+data_indiv[,5]+data_indiv[,6]+data_indiv[,7]+data_indiv[,8]+data_indiv[,9]+data_indiv[,10]+data_indiv[,11]+data_indiv[,12]+data_indiv[,13]+data_indiv[,14]+data_indiv[,15]+data_indiv[,16]+data_indiv[,17])^2)

summary(bayes_model3_without)
summary(bayes_model3)

barplot(bayes_model3$coefficients,names.arg=attr(bayes_model3$coefficients,"names"),las=2,ylim=c(-1,1),mar = c(10, 5, 4, 2))

#Significant are 64,77,89,123
barplot(c(bayes_model3$coefficients[64],bayes_model3$coefficients[77],bayes_model3$coefficients[89],bayes_model3$coefficients[123]),names.arg=c(attr(bayes_model3$coefficients,"names")[64],attr(bayes_model3$coefficients,"names")[77],attr(bayes_model3$coefficients,"names")[89],attr(bayes_model3$coefficients,"names")[123]),las=1,mar = c(10, 5, 4, 2))
#arrows(bayes_model2$coefficients, bayes_model2$coefficients - bayes_model2$deviance, bayes_model2$coefficients + bayes_model2$deviance, lwd = 1.5, angle = 90,
#       code = 3, length = 0.05)

AIC(bayes_model3)  ##-1524.738
stopifnot(all.equal(AIC(bayes_model3),
                    AIC(logLik(bayes_model3)))) ##917.3691 (df=155)
BIC(bayes_model3) ##-589.7223

AIC(bayes_model3_without)  ##-1597.221
stopifnot(all.equal(AIC(bayes_model3_without),
                    AIC(logLik(bayes_model3_without)))) ##'log Lik.' 817.6103 (df=19)
BIC(bayes_model3_without) ##-1482.606

#AIC(linear_model3, bayes_model3)
#BIC(linear_model3, bayes_model3)

```

## Bayesian estimates

```{r}
library(MCMCglmm)

set.seed(14)
#prior.m3 <- list(
#  R=list(V=1, n=1, fix=1),
#  G=list(G1=list(V        = diag(8),
#                 n        = 8,
#                 alpha.mu = rep(0, 8),
#                 alpha.V  = diag(8)*25^2),
#         G2=list(V        = diag(4),
#                 n        = 4,
#                 alpha.mu = rep(0, 4),
#                 alpha.V  = diag(4)*25^2)))
prior.m3 <- list()

#Markov chain Monte Carlo Sampler for Multivariate Generalised Linear Mixed Models with special emphasis on correlated random effects
#bayes_model4_without <- MCMCglmm(fixed = scores_indiv ~ V1+V2+V3+V4+V5+V6+V7+V8+V9+V10+V11+V12+V13+V14+V15+V16+V17, data = df,family="gaussian", prior  = prior.m3, thin   = 1, burnin = 7000, nitt   = 9000)

bayes_model4 <- MCMCglmm(fixed = scores_indiv ~ (V1+V2+V3+V4+V5+V6+V7+V8+V9+V10+V11+V12+V13+V14+V15+V16+V17)^2, data = df,family="gaussian", prior  = prior.m3, thin   = 1, burnin = 7000, nitt   = 9000)
summary(bayes_model4$Sol)
barplot(bayes_model4[[1]],las=2,mar = c(10, 5, 4, 2))

## Check the convergence of the burnin to nitt. If there seem to be non autocorrelated noise with same baseline (stationary), then it seems to converge
par(mfrow=c(3,2), mar=c(2,2,1,0))
plot(bayes_model4$Sol, auto.layout=F)


## Plot autocorrelation function to see if there is autocorrelated noise, but quantified from the traces above
plot.acfs <- function(x) {
  n <- dim(x)[2]
  
  par(mfrow=c(ceiling(n/4),2), mar=c(1,2,1,0))
  for (i in 1:(n/2)) {
    acf(x[,i], lag.max=100, main=colnames(x)[i])
    grid()
  }
  
  par(mfrow=c(ceiling(n/4),2), mar=c(1,2,1,0))
  for (i in ((n/2)+1):n) {
    acf(x[,i], lag.max=100, main=colnames(x)[i])
    grid()
  }
}
plot.acfs(bayes_model4$Sol)

## Check the traces one by one for each of the variables: from the burnin to nitt
trace.plots <- function(x) {
  n <- dim(x)[2]
  par(mfrow=c(ceiling(n/2),2), mar=c(0,0.5,1,0.5))
  for (i in 1:n) {
    plot(as.numeric(x[,i]), t="l", main=colnames(x)[i], xaxt="n", yaxt="n")
  }
}
trace.plots(bayes_model4$Sol)

## This is for running several in parallel to check if they always converge to the same values

#library(parallel)

#set.seed(14)
#bayes_model4_parallel <- mclapply(1:4, function(i) {
#MCMCglmm(fixed = scores_indiv ~  (V1+V2+V3+V4+V5+V6+V7+V8+V9+V10+V11+V12+V13+V14+V15+V16+V17)^2, data = df,family="gaussian", prior  = prior.m3, thin = 1, burnin = 3000, nitt = 4000)
#}, mc.cores=1)

#m6 <- lapply(bayes_model4_parallel, function(m) m$Sol)
#m6 <- do.call(mcmc.list, m6)

#summary(m6[[1]])
#summary(m6[[2]])
#summary(m6[[3]])
#summary(m6[[4]])

#barplot(m6[[1]],las=2,mar = c(10, 5, 4, 2))
#barplot(m6[[2]],las=2,mar = c(10, 5, 4, 2))
#barplot(m6[[3]],las=2,mar = c(10, 5, 4, 2))
#barplot(m6[[4]],las=2,mar = c(10, 5, 4, 2))

## Quantifying convergence
#library(coda)

#This plot shows the evolution of Gelman and Rubin's shrink factor as the number of iterations increases.The Markov chain is divided into bins according to the arguments bin.width and max.bins. Then the Gelman-Rubin shrink factor is repeatedly calculated. The first shrink factor is calculated with observations 1:50, the second with observations 1:(50+n) where n is the bin width, the third contains samples 1:(50+2n) and so on.
#par(mfrow=c(4,2), mar=c(2,2,1,2))
#gelman.plot(m6, auto.layout=F)


#The 'potential scale reduction factor' is calculated for each variable in x, together with upper and lower confidence limits. Approximate convergence is diagnosed when the upper limit is close to 1. For multivariate chains, a multivariate value is calculated that bounds above the potential scale reduction factor for any linear combination of the (possibly transformed) variables.The confidence limits are based on the assumption that the stationary distribution of the variable under examination is normal. Hence the 'transform' parameter may be used to improve the normal approximation.
#gelman.diag(m6)


# Posterior means and 95% credible intervals plot

plot.estimates <- function(x) {
  if (class(x) != "summary.mcmc")
    x <- summary(x)
  n <- dim(x$statistics)[1]
  par(mfrow=c(1,1),mar=c(2, 12, 4, 1))
  plot(x$statistics[,1], n:1,
       yaxt="n", ylab="",
       xlim=range(x$quantiles)*1.2,
       pch=19,
       main="Posterior means and 95% credible intervals")
  grid()
  axis(2, at=n:1, rownames(x$statistics), las=2)
  arrows(x$quantiles[,1], n:1, x$quantiles[,5], n:1, code=0)
  abline(v=0, lty=2)
}

plot.estimates(summary(bayes_model4[[1]]))

#The advantage of DIC over other criteria in the case of Bayesian model selection is that the DIC is easily calculated from the samples generated by a Markov chain Monte Carlo simulation. AIC and BIC require calculating the likelihood at its maximum over ?? {\displaystyle \theta \,} \theta \,, which is not readily available from the MCMC simulation. But to calculate DIC, simply compute D ¯ {\displaystyle {\bar {D}}} \bar{D} as the average of D ( ?? ) {\displaystyle D(\theta )\,} D(\theta )\, over the samples of ?? {\displaystyle \theta \,} \theta \,, and D ( ?? ¯ ) {\displaystyle D({\bar {\theta }})} D({\bar {\theta }}) as the value of D {\displaystyle D\,} D\, evaluated at the average of the samples of ?? {\displaystyle \theta \,} \theta \,. Then the DIC follows directly from these approximations. Claeskens and Hjort (2008, Ch. 3.5) show that the DIC is large-sample equivalent to the natural model-robust version of the AIC.
summary(bayes_model4) ## DIC: -1785.648 
#summary(bayes_model4_without) ## DIC: -1601.209
#AIC(bayes_model4)  ##
#stopifnot(all.equal(AIC(bayes_model4),
#                    AIC(logLik(bayes_model4)))) ##
#BIC(bayes_model4) ##


```
## See how the variables change their effect with the expression level

```{r}

# Boxplots for each neuronal cluster. It shows me that some neurons are linearly increasing their phenotype while others show other nonlinearities. It might be too litle amount of data but it can give hints where to look at. I could change light intensities for some of the lines to see how they affect to the behavioral phenotypes. According to Aso et al. 2012 they should be linear, but as I can see here there might be also nonlinear.

boxplot(df$scores_indiv ~ df$V1,main = rownames(DANs.valence)[1], xlab="Expression level",ylab="Score effect")
boxplot(df$scores_indiv ~ df$V2,main = rownames(DANs.valence)[2], xlab="Expression level",ylab="Score effect")
boxplot(df$scores_indiv ~ df$V3,main = rownames(DANs.valence)[3], xlab="Expression level",ylab="Score effect")
boxplot(df$scores_indiv ~ df$V4,main = rownames(DANs.valence)[4], xlab="Expression level",ylab="Score effect")
boxplot(df$scores_indiv ~ df$V5,main = rownames(DANs.valence)[5], xlab="Expression level",ylab="Score effect")
boxplot(df$scores_indiv ~ df$V6,main = rownames(DANs.valence)[6], xlab="Expression level",ylab="Score effect")
boxplot(df$scores_indiv ~ df$V7,main = rownames(DANs.valence)[7], xlab="Expression level",ylab="Score effect")
boxplot(df$scores_indiv ~ df$V8,main = rownames(DANs.valence)[8], xlab="Expression level",ylab="Score effect")
boxplot(df$scores_indiv ~ df$V9,main = rownames(DANs.valence)[9], xlab="Expression level",ylab="Score effect")
boxplot(df$scores_indiv ~ df$V10,main = rownames(DANs.valence)[10], xlab="Expression level",ylab="Score effect")
boxplot(df$scores_indiv ~ df$V11,main = rownames(DANs.valence)[11], xlab="Expression level",ylab="Score effect")
boxplot(df$scores_indiv ~ df$V12,main = rownames(DANs.valence)[12], xlab="Expression level",ylab="Score effect")
boxplot(df$scores_indiv ~ df$V13,main = rownames(DANs.valence)[13], xlab="Expression level",ylab="Score effect")
boxplot(df$scores_indiv ~ df$V14,main = rownames(DANs.valence)[14], xlab="Expression level",ylab="Score effect")
boxplot(df$scores_indiv ~ df$V15,main = rownames(DANs.valence)[15], xlab="Expression level",ylab="Score effect")
boxplot(df$scores_indiv ~ df$V16,main = rownames(DANs.valence)[16], xlab="Expression level",ylab="Score effect")
boxplot(df$scores_indiv ~ df$V17,main = rownames(DANs.valence)[17], xlab="Expression level",ylab="Score effect")

#plot( scores_indiv ~ df$V1)
#abline(lm(scores_indiv ~ df$V1), col="red", lwd = 3) # regression line (y~x)
#plot( scores_indiv ~ df$V2)
#abline(lm(scores_indiv ~ df$V2), col="red", lwd = 3) # regression line (y~x)
#plot( scores_indiv ~ df$V3)
#abline(lm(scores_indiv ~ df$V3), col="red", lwd = 3) # regression line (y~x)
#plot( scores_indiv ~ df$V4)
#abline(lm(scores_indiv ~ df$V4), col="red", lwd = 3) # regression line (y~x)

# Save in vector graphics
#svg(filename="Std_SVG.svg", 
#    width=5, 
#    height=4, 
#    pointsize=12)
#plot( scores_indiv ~ df$V17)
#dev.off()

#setEPS()
#postscript("whatever.eps")
#plot(rnorm(100), main="Hey Some Data")
#dev.off()
#postscript("foo.eps", horizontal = FALSE, onefile = FALSE, paper = "special", height = 10, width = 10)
#cairo_ps("image.eps")
#plot(1, 10)
#dev.off()

```

## Frequentist nonlinear estimates

```{r}


#*************************************************************Error************************************************************************

### Your call to nls fits by least squares, whereas glm fits by maximum

### likelihood.  Not the same thing: ml gives more weights to values with

### fitted values near zero or one.


non_linear_model <-nls(scores_indiv ~  (V1+V2+V3+V4+V5+V6+V7+V8+V9+V10+V11+V12+V13+V14+V15+V16+V17)^2, data = df,start=c(a=0,b=0,c=0,d=0,e=0,f=0,g=0,h=0,i=0),algorithm = "plinear")
#non_linear_model <-nls(scores_indiv ~  (y1(PAM,PPL1)+ped(PAM,PPL1)+a2(PPL1)+aPRIME2(PPL1)+aPRIME1(PPL1)+y2(PPL1,PAM)+a3(PPL1)+aPRIME3(PPL1)+b2(PAM)+bPRIME2(PAM)+b1(PAM)+a1(PAM)+bPRIME1(PAM)+y5(PAM)+y4(PAM)+y3(PAM)+ca(PPL2ab))^2, data = df,start=c(a=0,b=0,c=0,d=0,e=0,f=0,g=0,h=0,i=0),algorithm = "plinear")
non_linear_model <-nls(scores_indiv ~  V1, data = df,start=c(a=0,b=0,c=0,d=0,e=0,f=0,g=0,h=0,i=0),algorithm = "plinear")

#the error message suggests the matrix has no inverse as in A*A-1 =I can't be found- usually these things happen because the data is not a good fit to the model.
summary(non_linear_model)


#This generic function fits a nonlinear mixed-effects model in the formulation described in Lindstrom and Bates (1990) but allowing for nested random effects. The within-group errors are allowed to be correlated and/or have unequal variances.

fm1 <- nlme(scores_indiv ~ (V1+V2+V3+V4+V5+V6+V7+V8+V9+V10+V11+V12+V13+V14+V15+V16+V17)^2,fixed = df$scores_indiv ~ list(df$V1,df$V2,df$V3,df$V4,df$V5), data = df,start=c(a=0,b=0,c=0,d=0,e=0,f=0,g=0,h=0,i=0),verbose = TRUE)

summary(fm1)

#fm2 <- update(fm1, random = pdDiag(Asym + lrc ~ 1))
#summary(fm2)



#*************************************************************Error************************************************************************


```

## General additive model

```{r}

#gam is used to fit generalized additive models, specified by giving a symbolic description of the additive predictor and a description of the error distribution. gam uses the backfitting algorithm to combine different smoothing or fitting methods. The methods currently supported are local regression and smoothing splines.

#requiring the Package 
require(gam)

gam1<-gam(scores_indiv ~ (s(V1,df=4)+V2+s(V3,df=4)+s(V4,df=4)+s(V5,df=4)+s(V6,df=4)+V7+V8+s(V9,df=4)+s(V10,df=4)+V11+s(V12,df=4))^2,data = df)

gam2<-gam(scores_indiv ~ V1+V2+V3+V4+V5+V6+V7+V8+V9+V10+V11+V12+V13+V14+V15+V16+V17,data = df)

#in the above function s() is the shorthand for fitting smoothing splines 
#in gam() function

summary(gam1) ##AIC: -1763.784
summary(gam2) ##AIC: -1600.953

logLik(gam1) ##'log Lik.' 904.8917 (df=23)
logLik(gam2) ##'log Lik.' 818.4766 (df=18)

#gam.check(gam2)

barplot(gam1$coefficients)
barplot(gam2$coefficients)

#Plotting the Model
par(mfrow=c(1,3)) #to partition the Plotting Window
plot(gam1,se = TRUE) 
#se stands for standard error Bands

#Plotting the Model
par(mfrow=c(1,3)) #to partition the Plotting Window
plot(gam2,se = TRUE) 
#se stands for standard error Bands

#anova() function to test the goodness of fit and choose the best Model
#Using Chi-squared Non parametric Test due to Binary Classification Problem and categorical Target

anova(gam1,gam2,test = "Chisq")

lm_with_ns_without<-lm(scores_indiv ~ ns(V1,df=4)+V2+ns(V3,df=4)+ns(V4,df=4)+ns(V5,df=4)+ns(V6,df=4)+V7+V8+ns(V9,df=4)+ns(V10,df=4)+V11+ns(V12,df=4)+ns(V13,df=4)+V14+V15+V16+V17,data = df)
summary(lm_with_ns_without) ##Residual standard error: 0.1832 on 3057 degrees of freedom. Multiple R-squared:  0.2467,	Adjusted R-squared:  0.2415
AIC(lm_with_ns_without) ##-1690.126
BIC(lm_with_ns_without) ##-1551.382
logLik(lm_with_ns_without) ##log Lik.' 868.063 (df=23)

lm_with_ns<-lm(scores_indiv ~ (ns(V1,df=4)+V2+ns(V3,df=4)+ns(V4,df=4)+ns(V5,df=4)+ns(V6,df=4)+V7+V8+ns(V9,df=4)+ns(V10,df=4)+V11+ns(V12,df=4)+ns(V13,df=4)+V14+V15+V16+V17)^2,data = df)
#ns() is function used to fit a Natural Cubic Spline. One could also use bs()
summary(lm_with_ns) ##Residual standard error: 0.1787 on 3053 degrees of freedom. Multiple R-squared:  0.2842,	Adjusted R-squared:  0.2784 
AIC(lm_with_ns) ##-1839.437
BIC(lm_with_ns) ##-1676.564
logLik(lm_with_ns) ##'log Lik.' 946.7187 (df=27)
AIC(lm_with_ns,linear_model3) ##lm_with_ns is the best model so far

barplot(lm_with_ns$coefficients)

#Now plotting the Model

plot(lm_with_ns,se=T)

```



## Bayesian nonlinear estimates

```{r}

#*************************************************************Error************************************************************************

#library(brms)

#fit_zinb1 <- brm(scores_indiv ~  (V1+V2+V3+V4+V5+V6+V7+V8+V9+V10+V11+V12+V13+V14+V15+V16+V17)^2, data = df)
#family = zero_inflated_poisson("log"))

#summary(fit_zinb1)

#marginal_effects(fit_zinb1)

#*************************************************************Error************************************************************************


set.seed(14)
#prior.m3 <- list(
#  R=list(V=1, n=1, fix=1),
#  G=list(G1=list(V        = diag(8),
#                 n        = 8,
#                 alpha.mu = rep(0, 8),
#                 alpha.V  = diag(8)*25^2),
#         G2=list(V        = diag(4),
#                 n        = 4,
#                 alpha.mu = rep(0, 4),
#                 alpha.V  = diag(4)*25^2)))
prior.m3 <- list()

#Markov chain Monte Carlo Sampler for Multivariate Generalised Linear Mixed Models with special emphasis on correlated random effects
bayes_nonlinear_model <- MCMCglmm(fixed = scores_indiv ~ (ns(V1,df=4)+V2+ns(V3,df=4)+ns(V4,df=4)+ns(V5,df=4)+ns(V6,df=4)+V7+V8+ns(V9,df=4)+ns(V10,df=4)+V11+ns(V12,df=4)+ns(V13,df=4)+V14+V15+V16+V17)^2,data = df,family="gaussian", prior  = prior.m3, thin   = 1, burnin = 7000, nitt   = 9000)

summary(bayes_nonlinear_model) ##DIC: -1839.164
#summary(bayes_nonlinear_model$Sol)
barplot(bayes_nonlinear_model[[1]],las=2,mar = c(10, 5, 4, 2))
plot.estimates(summary(bayes_nonlinear_model[[1]]))

## Check the convergence of the burnin to nitt. If there seem to be non autocorrelated noise with same baseline (stationary), then it seems to converge
#par(mfrow=c(3,2), mar=c(2,2,1,0))
#plot(bayes_nonlinear_model$Sol, auto.layout=F)


## Plot autocorrelation function to see if there is autocorrelated noise, but quantified from the traces above
#plot.acfs(bayes_nonlinear_model$Sol)

## Check the traces one by one for each of the variables: from the burnin to nitt
#trace.plots(bayes_nonlinear_model$Sol)

#library(parallel)

#set.seed(14)
#bayes_nonlinear_model_parallel <- mclapply(1:4, function(i) {
#MCMCglmm(fixed = scores_indiv ~ (ns(V1,df=4)+V2+ns(V3,df=4)+ns(V4,df=4)+ns(V5,df=4)+ns(V6,df=4)+V7+V8+ns(V9,df=4)+ns(V10,df=4)+V11+ns(V12,df=4)+ns(V13,df=4)+V14+V15+V16+V17)^2, data = df,family="gaussian", prior  = prior.m3, thin = 1, burnin = 7000, nitt = 9000)
#}, mc.cores=1)

#m6_nonlinear <- lapply(bayes_nonlinear_model_parallel, function(m) m$Sol)
#m6_nonlinear <- do.call(mcmc.list, m6_nonlinear)

#summary(m6_nonlinear[[1]])
#summary(m6_nonlinear[[2]])
#summary(m6_nonlinear[[3]])
#summary(m6_nonlinear[[4]])

#barplot(m6_nonlinear[[1]],las=2,mar = c(10, 5, 4, 2))
#barplot(m6_nonlinear[[2]],las=2,mar = c(10, 5, 4, 2))
#barplot(m6_nonlinear[[3]],las=2,mar = c(10, 5, 4, 2))
#barplot(m6_nonlinear[[4]],las=2,mar = c(10, 5, 4, 2))

## Quantifying convergence
#library(coda)

#This plot shows the evolution of Gelman and Rubin's shrink factor as the number of iterations increases.The Markov chain is divided into bins according to the arguments bin.width and max.bins. Then the Gelman-Rubin shrink factor is repeatedly calculated. The first shrink factor is calculated with observations 1:50, the second with observations 1:(50+n) where n is the bin width, the third contains samples 1:(50+2n) and so on.
#par(mfrow=c(4,2), mar=c(2,2,1,2))
#gelman.plot(m6_nonlinear, auto.layout=F)


#The 'potential scale reduction factor' is calculated for each variable in x, together with upper and lower confidence limits. Approximate convergence is diagnosed when the upper limit is close to 1. For multivariate chains, a multivariate value is calculated that bounds above the potential scale reduction factor for any linear combination of the (possibly transformed) variables.The confidence limits are based on the assumption that the stationary distribution of the variable under examination is normal. Hence the 'transform' parameter may be used to improve the normal approximation.
#gelman.diag(m6_nonlinear)


# Posterior means and 95% credible intervals plot

#plot.estimates(summary(m6_nonlinear[[1]]))


```


## Save results

```{r}

#write.table(DANs.valence, "DANsvalence.txt", sep="\t", row.names = TRUE,col.names = FALSE)
#write.table(expected.model2, "expectedVSobservedmodel.txt", sep="\t", row.names = TRUE,col.names = TRUE)

```

